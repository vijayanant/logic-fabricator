# This file serves as a template for your .env file.
# Copy this to a new file named .env and fill in the values for your specific setup.

# --- Required Settings ---

# The provider for the LLM (e.g., "ollama", "openai")
LOGIC_FABRICATOR_PROVIDER=

# The specific model to use (e.g., "llama3.1", "gpt-4-turbo")
LOGIC_FABRICATOR_MODEL=

# The API key for the LLM provider.
# For local Ollama, this can be any non-empty string (e.g., "ollama").
LOGIC_FABRICATOR_API_KEY=


# --- Optional Settings ---

# The base URL for the LLM API. Only needed if it differs from the provider's default.
# For local Ollama, this is typically not needed if Ollama runs on the default localhost port.
# For Docker setups, it might be http://host.docker.internal:11434
# LOGIC_FABRICATOR_BASE_URL=


# --- Example for local Ollama ---
# LOGIC_FABRICATOR_PROVIDER=ollama
# LOGIC_FABRICATOR_MODEL=llama3.1
# LOGIC_FABRICATOR_API_KEY=ollama
# LOGIC_FABRICATOR_BASE_URL=http://host.docker.internal:11434/v1

# --- Example for OpenAI ---
# LOGIC_FABRICATOR_PROVIDER=openai
# LOGIC_FABRICATOR_MODEL=gpt-4-turbo
# LOGIC_FABRICATOR_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx