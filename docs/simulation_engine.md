# Simulation Engine

> *"Letâ€™s press play on logic space."*

---

## ðŸŽ¯ Purpose

The Simulation Engine is the orchestrator of the logic-space. It takes a `BeliefSystem` and a `WorldState` as input, and processes a sequence of natural-language statements. It evaluates each statement against the active rules in the `BeliefSystem`, determines consequences (both new `Statements` and `Effects` on the `WorldState`), detects contradictions, and records the evolution of logic.

Think of it as a turn-based logic interpreter â€” like running a story through a brain made of belief.

---

## ðŸ§ª What It Simulates

- **Statements**: Inputs like "Ravi trusts Alice" or "Anyone who is trusted may speak."
- **Rules**: The belief systemâ€™s logic (structured by LLM or the user)
- **State**: Any persistent derived values (trust levels, access flags, outcomes)

---

## ðŸ”„ Core Loop (MVP)

1. **Input Statement** (natural language)
2. **Structure Statement** (LLM or parser)
3. **Run Inference Loop**:
    - Get rules and statements from the `BeliefSystem`.
    - Apply rules to find consequences (`Statements` and `Effects`).
    - Apply the `Effects` to its `WorldState`.
    - Add the new `Statements` back to the `BeliefSystem`'s state.
    - Loop until no new consequences are generated.
4. **Check for Contradictions**
   - If found: pause, fork, or escalate
5. **Record All Activity** in MCP

---

## âš™ï¸ Handling Multiple Applicable Rules

It is common for a single statement to match multiple rules in a belief system. The Simulation Engine is designed to handle this gracefully.

### Scenario 1: Non-Contradictory Consequences

If multiple rules apply and their consequences do not contradict each other, all consequences are generated as new statements. The belief system simply becomes richer with more inferred facts.

**Example:**
- **Statement:** `Socrates is a human.`
- **Rule 1:** `If ?x is a human, then ?x is a mammal.`
- **Rule 2:** `If ?x is a human, then ?x has a name.`

The simulation generates two new statements: `Socrates is a mammal` and `Socrates has a name`.

### Scenario 2: Contradictory Consequences

If the consequences generated by applicable rules contradict each other, the Simulation Engine does not fail. Instead, it detects the conflict and triggers the **Contradiction Engine**. As per our design philosophy, this is a creative event, not an error. The default behavior is to **fork the Belief System**, creating separate, parallel realities where each conflicting rule can hold true.

---

## ðŸ§  Engine Behavior and Design Philosophy

Our simulation engine follows two important principles that are crucial for building predictable and powerful belief systems.

### Effect Idempotency

An `Effect` (a change to the `world_state`) should only be applied *once* for a given cause. Early versions of the engine had a "haunting" bug where a persistent fact (e.g., `"jon is dead"`) would re-trigger an associated effect (e.g., `decrement population`) on every new, unrelated simulation.

This has been corrected. The engine now has a persistent memory of which effects it has already applied for a specific set of causal bindings. It will not apply the same effect for the same reason again, ensuring that consequences are idempotent and predictable.

### Separation of Logic and State

A core design principle of the Fabricator is the separation between the "logical world" of `Statements` and the "physical world" of the `world_state` dictionary.

-   **`Statements` drive inference.** Rules are triggered *only* by the presence of other `Statements`.
-   **The `world_state` is a consequence.** It is a place for rules to store and modify data, but changes to the `world_state` do *not* trigger new rules. This prevents runaway feedback loops and keeps the chain of logic clear and traceable.

This leads to a question: what if we *want* a change in the world to be seen by other rules? For this, we use the **Dual Consequence Pattern**: a rule that needs to be observable has two consequences:
1.  An `Effect` that changes the `world_state`.
2.  A `Statement` that announces the change to the logical world.

This pattern makes the change to the physical world an observable fact, allowing other rules to react to it in a controlled and predictable way.

---

## ðŸ“¦ Input Format (Eventually)

```python
Statement(
  text="Ravi trusts Alice",
  subject="Ravi",
  verb="trusts",
  object="Alice"
)
```

These are run sequentially through a `Simulation.run()` method.

---

## ðŸ§  Output

The `Simulation.run()` method creates a `SimulationRecord` object with the following information, which is stored in the `simulation.mcp_records` list:

```python
class SimulationRecord:
    initial_statements: list[Statement]
    derived_facts: list[Statement]
    applied_rules: list[Rule]
    forked_belief_system: BeliefSystem | None
```

This lets us:

- Show the user what logic was triggered
- Let LLMs generate an explanation
- Persist outcomes for future branching

---

## ðŸ› ï¸ MVP Behavior

For the MVP:

- One statement at a time
- Simple rule matching (verb == verb)
- One belief system only
- Contradictions result in immediate forks (or errors)

Later:

- Batch statements
- Run scenarios with different forks
- Visual traces or timelines

---

## ðŸ¤ Integration Points

- **Rule Engine**: Uses `Rule.applies_to(statement)` to determine applicability.
- **Contradiction Engine**: Handles any conflict that emerges mid-simulation.
- **MCP**: Stores simulation results, versions, and forks.
- **LLM**: Parses statement, suggests rule interpretations, explains outcomes.

---

## ðŸ§ª Example Flow

```text
Input: "Ravi trusts Alice"

âœ”ï¸ Matches Rule 1: "If X trusts Y, then grant Y access"
â†’ Derived: "Alice has access"

âœ”ï¸ Matches Rule 2: "Nobody unverified gets access"
â†’ Contradiction detected
â†’ Fork into: belief_grants_access / belief_denies_access
â†’ Both beliefs now diverge
```

---

## ðŸ—ï¸ Next Steps

1. Implement a basic `Simulation.run()` method
2. Structure the return object with traceability in mind
3. Add contradiction detection
4. Later: batch simulation, branching scenarios, and user-facing visual logs

> *"The future doesnâ€™t unfold. Itâ€™s simulated."*

